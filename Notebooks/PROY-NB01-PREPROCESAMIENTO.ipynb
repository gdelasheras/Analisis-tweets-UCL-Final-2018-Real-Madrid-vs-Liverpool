{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto final. Datamining.\n",
    "### Análisis tweets UEFA Champions League Final 2018\n",
    "\n",
    "### Participantes:\n",
    "\n",
    "Gonzalo de las Heras de Matías - Jorge de la Fuente Tagarro - Alejandro Amarillas Cámara - Sergio Sampio Balmaseda.\n",
    "\n",
    "### Notebook (1/4). Preprocesamiento y transformación del dataset.\n",
    "\n",
    "### Objetivo del notebook:\n",
    "\n",
    "Este notebook se centra en preprocesar el dataset, limpiando las columnas existentes y generando aquellas nuevas que sean necesarias.<br><br>\n",
    "\n",
    "![title](Images/background.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "import sys\n",
    "import goslate\n",
    "from matplotlib import *\n",
    "from pylab import *\n",
    "from datetime import datetime, timedelta\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from collections import Counter\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import translate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(\"/Users/gonzalo/Documents/Keys/google.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descarga de datasets necesarios de nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('omw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcesarTweet(Tweet):\n",
    "    \"\"\"\n",
    "    Función para eliminar del texto del tweet: enlaces, tildes, # etc.\n",
    "    \"\"\"\n",
    "    Tweet = re.sub(r\"\\&\\w*;\", \"\", Tweet)\n",
    "    Tweet = re.sub(\"@[^\\s]+\",\"\",Tweet)\n",
    "    Tweet = re.sub(\"\\[[^\\s]+\\]\",\"\",Tweet)\n",
    "    Tweet = re.sub(r\"\\$\\w*\", \"\", Tweet)\n",
    "    Tweet = Tweet.lower()\n",
    "    Tweet = re.sub(r\"https?:\\/\\/.*\\/\\w*\", \"\", Tweet)\n",
    "    Tweet = re.sub(r\"https...\", \"\", Tweet)\n",
    "    Tweet = re.sub(r\"á\", \"a\", Tweet)\n",
    "    Tweet = re.sub(r\"é\", \"e\", Tweet)\n",
    "    Tweet = re.sub(r\"í\", \"i\", Tweet)\n",
    "    Tweet = re.sub(r\"ó\", \"o\", Tweet)\n",
    "    Tweet = re.sub(r\"ú\", \"u\", Tweet)\n",
    "    Tweet = re.sub(r\"#\\w*\", \"\", Tweet)\n",
    "    Tweet = re.sub(r\"[\" + string.punctuation.replace(\"@\", \"\") + \"]+\", \" \", Tweet)\n",
    "    Tweet = re.sub(r\"\\b\\w{1,2}\\b\", \"\", Tweet)\n",
    "    Tweet = re.sub(r\"\\s\\s+\", \" \", Tweet)\n",
    "    Tweet = Tweet.lstrip(\" \") \n",
    "    Tweet = \"\".join(c for c in Tweet if c <= \"\\uFFFF\") \n",
    "    return Tweet\n",
    "\n",
    "def PreprocesarTexto(Texto):\n",
    "    \"\"\"\n",
    "    Función para eliminar palabras que no afectan al análisis de sentimiento y signos de puntuación.\n",
    "    \"\"\"\n",
    "    nopunc = [char for char in list(Texto) if char not in string.punctuation]\n",
    "    nopunc = \"\".join(nopunc)\n",
    "    return [word for word in nopunc.lower().split() if word.lower() not in stopwords.words(\"english\")]\n",
    "\n",
    "def CodificarPalabra(Palabra):\n",
    "    \"\"\"\n",
    "    Función para codificar una palanra como ASCII.\n",
    "    \"\"\"\n",
    "    return Palabra.encode(\"ascii\", errors=\"ignore\").decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_DATASET_JSON = \"/Users/gonzalo/Documents/Datasets/tweets-ucl-final-2018/data.json\"\n",
    "if \"json\" not in globals():\n",
    "    json = pd.read_json(DIR_DATASET_JSON, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Construcción dataset final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.copy()\n",
    "Dataset_Final = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primera fase del proceso de KDD en el que limpiamos los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Eliminación de columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(data[data['coordinates'] == None].index) == 0:\n",
    "    print(\"La columna 'coordinates' está toda a NaN, borrando columna.\")\n",
    "    del data['coordinates']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(data[data['contributors'] == data['contributors']].index) == 0:\n",
    "    print(\"La columna 'contributors' está toda a NaN, borrando columna.\")\n",
    "\n",
    "del data['contributors']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Withheld_in_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withheld_in_countries = len(data[data['withheld_in_countries'] == data['withheld_in_countries']].index)\n",
    "if  withheld_in_countries== 0:\n",
    "    print(\"La columna 'withheld_in_countries' está toda a NaN, borrando columna.\")\n",
    "    del data['withheld_in_countries']\n",
    "else:\n",
    "     print('Hay ', withheld_in_countries, \"ocurrencias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay muy pocas ocurrencias que no sean nulas, eliminamos la columna entera al no ser de utilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['withheld_in_countries']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 favorite_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "favorite_count = len(data[data['favorite_count'] != 0.0].index)\n",
    "if  favorite_count == 0:\n",
    "    print(\"La columna 'favorite_count' está toda a 0.0\")\n",
    "    del data['favorite_count']\n",
    "else:\n",
    "    print('Hay ', favorite_count, \"ocurrencias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5 geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_count = len(data[data['geo'] == None].index)\n",
    "if geo_count == 0:\n",
    "    print(\"La columna 'geo' está toda a NaN, borrando columna.\")\n",
    "    del data['geo']\n",
    "else:\n",
    "    print('Hay ', geo_count, \"ocurrencias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.6 filter_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_level_count = len(data['filter_level'].unique().tolist())\n",
    "if filter_level_count == 1:\n",
    "    print(\"La columna 'filter_level' es siempre igual.\")\n",
    "    del data['filter_level']\n",
    "else:\n",
    "    print('Hay ', filter_level_count, \"valores distintos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.6 place (se recogerá la información en la parte de usuarios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de haber probado unos cuentos, nos quedamos con la localizacion de los usurios, casi siempre es la misma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['place']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.8 Campos que no aportan información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['display_text_range']\n",
    "del data['truncated']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Eliminación filas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Eliminación filas nulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data[data['id'] != data['id']].id.index)\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "Dataset_Final['id'] = data['id'].astype('int64')\n",
    "del data['id']\n",
    "del data['id_str']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Volcado al dataset final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Conversión de fecha completa a solo tiempo (todos los tweets son del mismo día)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['created_at'] += pd.DateOffset(hours=2) \n",
    "Dataset_Final['hora'] = data['created_at']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Formateo de fuente (plataforma desde el que se lanza el tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraemos la fuente que se encuentra como el literal de un hipervínculo.\n",
    "Dataset_Final['fuente'] = data['source'].replace('<a[^>]*>([^<]+)<\\/a>', r'\\1', regex=True)\n",
    "\n",
    "# Categorizamos las funtes más importantes.\n",
    "Dataset_Final.loc[Dataset_Final['fuente'] == 'Twitter for Android', 'fuente'] = 'Android'\n",
    "Dataset_Final.loc[Dataset_Final['fuente'] == 'Twitter for iPhone', 'fuente']  = 'iPhone'\n",
    "Dataset_Final.loc[Dataset_Final['fuente'] == 'Twitter for iPad', 'fuente']  = 'iPad'\n",
    "Dataset_Final.loc[Dataset_Final['fuente'] == 'Twitter Web Client', 'fuente']  = 'Twitter Web'\n",
    "del data['source']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Idioma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_Final['idioma'] = data['lang']\n",
    "del data['lang']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4 Id tweet respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['in_reply_to_status_id'].fillna(-1)\n",
    "Dataset_Final['tweet_respuesta_id'] = data['in_reply_to_status_id']\n",
    "del data['in_reply_to_status_id']\n",
    "del data['in_reply_to_status_id_str']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.5 Id usuario tweet respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['in_reply_to_user_id'].fillna(-1)\n",
    "Dataset_Final['tweet_respuesta_usuario_id'] = data['in_reply_to_user_id']\n",
    "del data['in_reply_to_user_id']\n",
    "del data['in_reply_to_user_id_str']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.6 Nombre usuario tweet respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['in_reply_to_screen_name'].fillna(-1)\n",
    "Dataset_Final['tweet_respuesta_nombre_twitter'] = data['in_reply_to_screen_name']\n",
    "del data['in_reply_to_screen_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.7 Información anidada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_usuarios = []\n",
    "name = [] \n",
    "screen_name = []\n",
    "description = []\n",
    "verified = []\n",
    "location = []\n",
    "followers_count = []\n",
    "listed_count = []\n",
    "favourites_count = []\n",
    "created_at = []\n",
    "usuario_lang = []\n",
    "statuses_count = []\n",
    "text = []\n",
    "hashtag = []\n",
    "user_mentions_screen_name = []\n",
    "user_mentions_id = []\n",
    "user_mentions_name = []\n",
    "\n",
    "# Iteración sobre todos los tweets.\n",
    "for index, row in data.iterrows():\n",
    "    \n",
    "    # Extraemos los campos necesarios.\n",
    "    datos_usuario = row[\"user\"]\n",
    "    extended_tweet = row[\"extended_tweet\"]\n",
    "    entities = row[\"entities\"] \n",
    "    statuses_count.append(datos_usuario[\"statuses_count\"])\n",
    "    location.append(datos_usuario[\"location\"])\n",
    "    id_usuarios.append(datos_usuario[\"id\"])\n",
    "    name.append(datos_usuario[\"name\"])\n",
    "    screen_name.append(datos_usuario[\"screen_name\"])\n",
    "    description.append(datos_usuario[\"description\"])\n",
    "    verified.append(datos_usuario[\"verified\"])\n",
    "    followers_count.append(datos_usuario[\"followers_count\"])\n",
    "    favourites_count.append(datos_usuario[\"favourites_count\"])\n",
    "    created_at.append(datos_usuario[\"created_at\"])\n",
    "    usuario_lang.append(datos_usuario[\"lang\"])\n",
    "    \n",
    "    # Menciones en un tweet.\n",
    "    user_mentions_screen_name_temp = []\n",
    "    user_mentions_id_temp = []\n",
    "    user_mentions_name_temp = []\n",
    "    \n",
    "    # Menciones.\n",
    "    for user in entities[\"user_mentions\"]:\n",
    "        user_mentions_screen_name_temp.append(user[\"screen_name\"])\n",
    "        user_mentions_id_temp.append(user[\"id\"])\n",
    "        user_mentions_name_temp.append(user[\"name\"])\n",
    "        \n",
    "    # Screename mención.\n",
    "    if len(user_mentions_screen_name_temp) > 0:\n",
    "        user_mentions_screen_name.append(user_mentions_screen_name_temp)\n",
    "    else:\n",
    "        user_mentions_screen_name.append(-1)\n",
    "        \n",
    "    # Id mención.\n",
    "    if len(user_mentions_id_temp) > 0:\n",
    "        user_mentions_id.append(user_mentions_id_temp)\n",
    "    else:\n",
    "        user_mentions_id.append(-1)\n",
    "        \n",
    "    # Nombre mención.\n",
    "    if len(user_mentions_name_temp) > 0:\n",
    "        user_mentions_name.append(user_mentions_name_temp)\n",
    "    else:\n",
    "        user_mentions_name.append(-1)\n",
    "    \n",
    "    # Existe la entidad extendida.\n",
    "    if extended_tweet is not \"nan\":\n",
    "        text.append(row[\"text\"])\n",
    "        hashtags_temp = []\n",
    "        for i in range(0, len(entities[\"hashtags\"])):\n",
    "            temp_hashtag = CodificarPalabra(entities[\"hashtags\"][i][\"text\"].lower())\n",
    "            if len(temp_hashtag) > 0:\n",
    "                hashtags_temp.append(temp_hashtag)\n",
    "        if len(hashtags_temp) > 0:\n",
    "            hashtag.append(hashtags_temp)\n",
    "        else:\n",
    "            hashtag.append(-1)\n",
    "    else:\n",
    "        text.append(extended_tweet[\"full_text\"])\n",
    "        hashtags_temp = []\n",
    "        for i in range(0, len(extended_tweet[\"entities\"][\"hashtags\"])):\n",
    "               hashtags_temp.append(extended_tweet[\"entities\"][\"hashtags\"][i][\"text\"].lower())\n",
    "        if len(hashtags_temp) > 0:\n",
    "            hashtag.append(hashtags_temp)\n",
    "        else:\n",
    "            hashtag.append(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Colocamos cada lista en una columna del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_Final['usuario_id'] = id_usuarios\n",
    "Dataset_Final['usuario_nombre'] = name\n",
    "Dataset_Final['usuario_nombre_twitter'] = screen_name\n",
    "Dataset_Final['usuario_localizacion'] = location\n",
    "Dataset_Final['usuario_idioma'] = usuario_lang\n",
    "Dataset_Final['usuario_verificado'] = verified\n",
    "Dataset_Final['usuario_numero_seguidores'] = followers_count\n",
    "Dataset_Final['usuario_numero_favoritos_hechos'] = favourites_count\n",
    "Dataset_Final['usuario_numero_tweets'] = statuses_count\n",
    "Dataset_Final['usuario_numero_creacion'] = created_at\n",
    "Dataset_Final['hashtag'] = hashtag\n",
    "Dataset_Final['texto_original'] = text\n",
    "Dataset_Final['mencion_usuario_id'] = user_mentions_id\n",
    "Dataset_Final['mencion_usuario_nombre'] = user_mentions_name\n",
    "Dataset_Final['mencion_usuario_nombre_twitter'] = user_mentions_screen_name\n",
    "\n",
    "del data['user']\n",
    "del data['entities']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rellenamos campos nulos y convertimos las columnas numéricas a int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Dataset_Final.fillna(-1, inplace=True)\n",
    "Dataset_Final.tweet_respuesta_id = Dataset_Final.tweet_respuesta_id.astype(int)\n",
    "Dataset_Final.tweet_respuesta_usuario_id = Dataset_Final.tweet_respuesta_usuario_id.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformación y generación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Limpiamos el texto de los tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_Final['texto_limpio'] = Dataset_Final['texto_original'].apply(ProcesarTweet)\n",
    "Dataset_Final['texto_traducido'] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Traducimos los textos de los tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "translate_client = translate.Client(credentials=credentials)\n",
    "terminado = False\n",
    "print(\"Hay\", len(Dataset_Final), \"filas\")\n",
    "for index, row in Dataset_Final.iterrows():\n",
    "    if index % 100 == 0:\n",
    "        print(index, \"filas traducidas.\")\n",
    "    if row[\"idioma\"] != \"en\" and row[\"texto_traducido\"] == \"-1\":\n",
    "        # Traducción al inglés.\n",
    "        target = 'en'\n",
    "        # Traducción.\n",
    "        translation = translate_client.translate(row[\"texto_limpio\"], target_language=target)\n",
    "        Dataset_Final.loc[index, 'texto_traducido'] = translation['translatedText']\n",
    "    else:\n",
    "        Dataset_Final.loc[index, 'texto_traducido'] = row['texto_limpio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Analizamos el sentimiento del tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizamos el tweet en un array de características.\n",
    "vectorizador = TfidfVectorizer(max_df=0.5, max_features=10000, min_df=7, stop_words='english', use_idf=True)\n",
    "vectorizador.fit_transform(Dataset_Final['texto_traducido'].str.upper())\n",
    "\n",
    "# Objeto analizador de sentimientos.\n",
    "analizador = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Analizamos el sentimiento.\n",
    "Dataset_Final['sentimiento_compound_polarity'] = Dataset_Final[\"texto_traducido\"] \\\n",
    "    .apply(lambda x:analizador.polarity_scores(x)['compound'])\n",
    "Dataset_Final['sentimiento_neutral'] = Dataset_Final[\"texto_traducido\"] \\\n",
    "    .apply(lambda x:analizador.polarity_scores(x)['neu'])\n",
    "Dataset_Final['sentimiento_negativo'] = Dataset_Final[\"texto_traducido\"] \\\n",
    "    .apply(lambda x:analizador.polarity_scores(x)['neg'])\n",
    "Dataset_Final['sentimiento_positivo'] = Dataset_Final[\"texto_traducido\"] \\\n",
    "    .apply(lambda x:analizador.polarity_scores(x)['pos'])\n",
    "    \n",
    "# Clasificación del resultado.\n",
    "Dataset_Final['sentimiento_tipo'] = \"\"\n",
    "Dataset_Final.loc[Dataset_Final['sentimiento_compound_polarity'] > 0 ,'sentimiento_tipo'] = 'POSITIVE'\n",
    "Dataset_Final.loc[Dataset_Final['sentimiento_compound_polarity'] == 0,'sentimiento_tipo'] = 'NEUTRAL'\n",
    "Dataset_Final.loc[Dataset_Final['sentimiento_compound_polarity'] < 0 ,'sentimiento_tipo'] = 'NEGATIVE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Separamos el texto del tweet en palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_Final['tokens'] = Dataset_Final['texto_traducido'].apply(PreprocesarTexto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Eliminamos hashtags no válidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_Final= Dataset_Final.reset_index(drop=True)\n",
    "for index, row in Dataset_Final.iterrows():\n",
    "    hashtags = []\n",
    "    print(index)\n",
    "    if row[\"hashtag\"] != \"-1\" and row[\"hashtag\"] != -1:\n",
    "        for hastag in row[\"hashtag\"]:\n",
    "            if hastag != \"_\" and hastag != \"__\" and hastag != \"___\" and hastag != \"uclfinal\":\n",
    "                hashtags.append(hastag)\n",
    "    if len(hashtags) > 0:\n",
    "        Dataset_Final.set_value(index, \"hashtag\", hashtags)\n",
    "    else:\n",
    "        Dataset_Final.loc[index, \"hashtag\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_Final.to_json(\"_datos_limpios.json\")\n",
    "Dataset_Final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>https://www.kaggle.com/xvivancos/tweets-during-r-madrid-vs-liverpool-ucl-2018</li>\n",
    "    <li>https://github.com/pbugnion/gmaps</li>\n",
    "    <li>https://pandas.pydata.org/</li>\n",
    "    <li>Apuntes de la asignatura</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
